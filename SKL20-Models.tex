\documentclass[SKL-MASTER.tex]{subfiles}
\begin{document}
\Large
%================================================================================ %
\section{Working with Linear Models}
In this chapter, we will cover the following topics:
\begin{itemize}
\item Fitting a line through data
\item Evaluating the linear regression model
\item Using ridge regression to overcome linear regression's shortfalls
\item Optimizing the ridge regression parameter
\item Using sparsity to regularize models
\item Taking a more fundamental approach to regularization with LARS
\item Using linear methods for classification â€“ logistic regression
\item Directly applying Bayesian ridge regression
\item Using boosting to learn from errors
\end{itemize}
\subsection{Introduction}
Linear models are fundamental in statistics and machine learning. Many methods rely on
a linear combination of variables to describe the relationship in the data. Quite often, great
efforts are taken in an attempt to make the transformations necessary so that the data can
be described in a linear combination.
In this chapter, we build up from the simplest idea of fitting a straight line through data to
classification, and finally to Bayesian ridge regression.
%========================================================%
% % - Working with Linear Models
% % - 56
\end{document}