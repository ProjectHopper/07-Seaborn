{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% % - http://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "% % - http://www.astroml.org/sklearn_tutorial/dimensionality_reduction.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is an often-used tool in astronomy and other data-intensive sciences. In a sense, it automates the trial-and-error process discussed in the previous section, and finds the most interesting linear combinations of attributes, so that high-dimensional data can be visualized in a 2D or 3D plot. Scikit-learn has methods to compute PCA and several variants. \n",
    "\n",
    "Classic PCA (sklearn.decomposition.PCA) is based on an eigenvalue decomposition of the data covariance, so that for N points, the computational cost grows as $\\mathcal{O}[N^3]$. This means that for large datasets like the current one, the fit can be very slow. You can try it as follows, but the computation may take up to several minutes for this dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f8b2daacf6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_projected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# warning: this takes a long time!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "X = normalize(data['X'])\n",
    "X_projected = pca.fit_transform(X)  # warning: this takes a long time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fortunately, scikit-learn has an alternative method that is much faster. The speed comes at a price: it is based on random projections, so the results are not as robust as the normal method. But for tasks such as ours where we are seeking only a few of a large number of eigenvectors, it performs fairly well. To keep our results consistent between runs, we’ll explicitly set the random seed for the fit. You should repeat this with several different random seeds to convince yourself that the results are consistent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import RandomizedPCA\n",
    "rpca = RandomizedPCA(n_components=4, random_state=0)\n",
    "X_proj = rpca.fit_transform(X)\n",
    "X_proj.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%===================================================================================== %\n",
    "% - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "sklearn.decomposition.PCA\n",
    "class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Principal component analysis (PCA)\n",
    "\n",
    "Linear dimensionality reduction using \\textit{Singular Value Decomposition} of the data and keeping only the most significant singular vectors to project the data to a lower dimensional space.\n",
    "\n",
    "This implementation uses the \\texttt{scipy.linalg} implementation of the singular value decomposition. It only works for dense arrays and is not scalable to large dimensional data.\n",
    "The time complexity of this implementation is O(n ** 3) assuming n ~ n_samples ~ n_features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:}\t\n",
    "\n",
    "* n_components : int, None or string\n",
    "Number of components to keep. if n_components is not set all components are kept:\n",
    "n_components == min(n_samples, n_features)\n",
    "if $n _{components}$ == ‘mle’, Minka’s MLE is used to guess the dimension if $0 < n_components < 1$, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components\n",
    "* copy : bool\n",
    "If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.\n",
    "* whiten : bool, optional\n",
    "When True (False by default) the components_ vectors are divided by n_samples times singular values to ensure uncorrelated outputs with unit component-wise variances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-fea20a91f68c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-fea20a91f68c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making there data respect some hard-wired assumptions.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making there data respect some hard-wired assumptions.\n",
    "Attributes:\t\n",
    "`components_` : array, [n_components, n_features]\n",
    "Components with maximum variance.\n",
    "`explained_variance_ratio_` : array, [n_components]\n",
    "Percentage of variance explained by each of the selected components. k is not set then all components are stored and the sum of explained variances is equal to 1.0\n",
    "`mean_` : array, [n_features]\n",
    "Per-feature empirical mean, estimated from the training set.\n",
    "`n_components_` : int\n",
    "The estimated number of components. Relevant when n_components is set to ‘mle’ or a number between 0 and 1 to select using explained variance.\n",
    "`noise_variance_` : float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. \n",
    "*See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to computed the estimated data covariance and score samples.*\n",
    "\n",
    "* See also ProbabilisticPCA, RandomizedPCA, KernelPCA, SparsePCA, TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "For $n_{components}=\"mle\"$, this class uses the method of Thomas P. Minka: Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\n",
    "\n",
    "Implements the probabilistic PCA model from: M. Tipping and C. Bishop, Probabilistic Principal Component Analysis, Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622 via the score and score_samples methods. See http://www.miketipping.com/papers/met-mppca.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to implementation subtleties of the Singular Value Decomposition (SVD), which is used in this implementation, running fit twice on the same matrix can lead to principal components with signs flipped (change in direction). For this reason, it is important to always use the same estimator object to transform data in a consistent fashion.\n",
    "#### Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99244289  0.00755711]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "* ``fit(X[, y])``\tFit the model with X.\n",
    "* ``fit_transform(X[, y])``\tFit the model with X and apply the dimensionality reduction on X.\n",
    "* ``get_covariance()``\tCompute data covariance with the generative model.\n",
    "* ``get_params([deep])``\tGet parameters for this estimator.\n",
    "* ``get_precision()``\tCompute data precision matrix with the generative model.\n",
    "* ``inverse_transform(X)``\tTransform data back to its original space, i.e.,\n",
    "* ``score(X[, y])``\tReturn the average log-likelihood of all samples\n",
    "* ``score_samples(X)``\tReturn the log-likelihood of each sample\n",
    "* ``set_params(**params)``\tSet the parameters of this estimator.\n",
    "* ``transform(X)``\tApply the dimensionality reduction on X.\n",
    "* ``__init__(n_components=None, copy=True, whiten=False)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-a93183d8fe74>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a93183d8fe74>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Fit the model with X.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### 1. ``fit(X, y=None)``\n",
    "\n",
    "Fit the model with X.\n",
    "\n",
    "### Parameters:\t\n",
    "X: array-like, shape (n_samples, n_features) :\n",
    "Training data, where n_samples in the number of samples and n_features is the number of features.\n",
    "\n",
    "### Returns:\t\n",
    "self : object\n",
    "Returns the instance itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ``fit_transform(X, y=None)``\n",
    "Fit the model with X and apply the dimensionality reduction on X.\n",
    "Parameters:\t\n",
    "X : array-like, shape (n_samples, n_features)\n",
    "Training data, where n_samples is the number of samples and n_features is the number of features.\n",
    "Returns:\t\n",
    "X_new : array-like, shape (n_samples, n_components)\n",
    "\n",
    "### 3. ``get_covariance()``\n",
    "Compute data covariance with the generative model.\n",
    "cov = components_.T * S**2 * components_ + sigma2 * eye(n_features) where S**2 contains the explained variances.\n",
    "Returns:\t\n",
    "cov : array, shape=(n_features, n_features)\n",
    "    Estimated covariance of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4. ``get_params(deep=True)``\n",
    "Get parameters for this estimator.\n",
    "Parameters:\t\n",
    "deep: boolean, optional :\n",
    "If True, will return the parameters for this estimator and contained subobjects that are estimators.\n",
    "Returns:\t\n",
    "params : mapping of string to any\n",
    "Parameter names mapped to their values.\n",
    "\n",
    "### 5. ``get_precision()``\n",
    "Compute data precision matrix with the generative model.\n",
    "Equals the inverse of the covariance but computed with the matrix inversion lemma for efficiency.\n",
    "Returns:\t\n",
    "precision : array, shape=(n_features, n_features)\n",
    "Estimated precision of data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ``inverse_transform(X)``\n",
    "Transform data back to its original space, i.e., return an input X_original whose transform would be X\n",
    "Parameters:\t\n",
    "X : array-like, shape (n_samples, n_components)\n",
    "New data, where n_samples is the number of samples and n_components is the number of components.\n",
    "Returns:\t\n",
    "X_original array-like, shape (n_samples, n_features) :\n",
    "Notes\n",
    "If whitening is enabled, inverse_transform does not compute the exact inverse operation as transform.\n",
    "\n",
    "### 4. ``score(X, y=None)``\n",
    "Return the average log-likelihood of all samples\n",
    "See. “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf\n",
    "Parameters:\t\n",
    "X: array, shape(n_samples, n_features) :\n",
    "The data.\n",
    "Returns:\t\n",
    "ll: float :\n",
    "Average log-likelihood of the samples under the current model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ``score_samples(X)``\n",
    "Return the log-likelihood of each sample\n",
    "See. “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf\n",
    "Parameters:\t\n",
    "X: array, shape(n_samples, n_features) :\n",
    "The data.\n",
    "Returns:\t\n",
    "ll: array, shape (n_samples,) :\n",
    "Log-likelihood of each sample under the current model\n",
    "\n",
    "### 4. ``set_params(**params)``\n",
    "Set the parameters of this estimator.\n",
    "The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.\n",
    "Returns:\tself :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``transform(X)``\n",
    "Apply the dimensionality reduction on X.\n",
    "X is projected on the first principal components previous extracted from a training set.\n",
    "\n",
    "#### Parameters:\t\n",
    "X : array-like, shape (n_samples, n_features)\n",
    "New data, where n_samples is the number of samples and n_features is the number of features.\n",
    "\n",
    "#### Returns:\t\n",
    "X_new : array-like, shape (n_samples, n_components)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\\subsection*{PCA example with Iris Data-set}\n",
    "Principal Component Analysis applied to the Iris dataset.\n",
    "%See here for more information on this dataset.\n",
    "\\begin{figure}\n",
    "\\centering\n",
    "\\includegraphics[width=0.7\\linewidth]{pcairis}\n",
    "\\caption{}\n",
    "\\label{fig:pcairis}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Python source code: plot_pca_iris.py\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "ax.text3D(X[y == label, 0].mean(),\n",
    "X[y == label, 1].mean() + 1.5,\n",
    "X[y == label, 2].mean(), name,\n",
    "horizontalalignment='center',\n",
    "bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral)\n",
    "\n",
    "x_surf = [X[:, 0].min(), X[:, 0].max(),\n",
    "X[:, 0].min(), X[:, 0].max()]\n",
    "y_surf = [X[:, 0].max(), X[:, 0].max(),\n",
    "X[:, 0].min(), X[:, 0].min()]\n",
    "x_surf = np.array(x_surf)\n",
    "y_surf = np.array(y_surf)\n",
    "v0 = pca.transform(pca.components_[0])\n",
    "v0 /= v0[-1]\n",
    "v1 = pca.transform(pca.components_[1])\n",
    "v1 /= v1[-1]\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
